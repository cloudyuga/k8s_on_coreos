# Keys setup.

Lets Create the required directory and copy the previously generated keys in that directory. Keys and Certificates stored in the `/etc/kubernetes/ssl` are used by the kubelet service and other kubernetes cluster component.
```
$ sudo mkdir -p /etc/kubernetes/ssl
$ cp /home/core/ca.pem apiserver.pem apiserver-key.pem /etc/kubernetes/ssl/.
$ ls /etc/kubernetes/ssl
```
Set proper permission for private key.
```
$ sudo chmod 600 /etc/kubernetes/ssl/*-key.pem
$ sudo chown root:root /etc/kubernetes/ssl/*-key.pem
```
# Network Setup.

Flannel provides a software defined overlay network for routing traffic to/from pods. Next up is to configure the flannel networking to allow Kubernetes to assign virtual IP addresses to pods.

Make a new directory for storing flannel options.
```
sudo mkdir -p /etc/flannel/

```


Then create an options file with the following command and save the content shown below.  In following content
- Replace `174.138.76.103` in following command with the Public IP address of `Master Node`.
- Replace `174.138.76.107` in following command with the Public IP address of `Worker Node1`.
- Replace `174.138.76.108` in following command with the Public IP address of `Worker Node2`.

```
$ sudo vi /etc/flannel/options.env

FLANNELD_IFACE=174.138.76.103
FLANNELD_ETCD_ENDPOINTS=http://174.138.76.103:2379,http://174.138.76.107:2379,http://174.138.76.108:2379
```

Create a systemd drop-in, which is a method for appending or overriding parameters of a systemd unit. In this case we're appending two dependency rules. Create the following drop-in, which will use the above configuration when flannel starts.

```
$ sudo mkdir -p /etc/systemd/system/flanneld.service.d/
$ sudo vi /etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf

[Service]
ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
```

# Docker Configuration

In order for flannel to manage the pod network in the cluster, Docker needs to be configured to use it. All we need to do is require that flanneld is running prior to Docker starting.

```
$ sudo mkdir -p /etc/systemd/system/docker.service.d/
$ sudo vi /etc/systemd/system/docker.service.d/40-flannel.conf

[Unit]
Requires=flanneld.service
After=flanneld.service
[Service]
EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
```

Create the Docker CNI Options file.
```
$ sudo mkdir -p /etc/kubernetes/cni/
$ sudo vi /etc/kubernetes/cni/docker_opts_cni.env

DOCKER_OPT_BIP=""
DOCKER_OPT_IPMASQ=""
```
We are using Flannel for networking so lets setup the Flannel CNI configuration as per shown below.
```
$ sudo mkdir /etc/kubernetes/cni/net.d/
$ sudo vi /etc/kubernetes/cni/net.d/10-flannel.conf

{
    "name": "podnet",
    "type": "flannel",
    "delegate": {
        "isDefaultGateway": true
    }
}
```
# Create the kubelet service.
- Replace `174.138.76.103` in following command with the Public IP address of `Master Node`.

```
$ sudo vi /etc/systemd/system/kubelet.service

[Service]
Environment=KUBELET_IMAGE_TAG=v1.6.1_coreos.0
Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
  --volume var-log,kind=host,source=/var/log \
  --mount volume=var-log,target=/var/log \
  --volume dns,kind=host,source=/etc/resolv.conf \
  --mount volume=dns,target=/etc/resolv.conf"
ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
ExecStartPre=/usr/bin/mkdir -p /var/log/containers
ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
ExecStart=/usr/lib/coreos/kubelet-wrapper \
  --api-servers=http://127.0.0.1:8080 \
  --register-schedulable=false \
  --cni-conf-dir=/etc/kubernetes/cni/net.d \
  --network-plugin=${NETWORK_PLUGIN} \
  --container-runtime=docker \
  --allow-privileged=true \
  --pod-manifest-path=/etc/kubernetes/manifests \
  --hostname-override=174.138.76.103 \
  --cluster_dns=10.3.0.10 \
  --cluster_domain=cluster.local
ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```
# Set Up the kubernetes services.

Then make a new directory for Kubernetes manifests which define the rest of the services that run on the master node.
```
$ sudo mkdir -p /etc/kubernetes/manifests/
```
### Set Up the kube-apiserver Pod.

So Now create kube-apiserver Pod In following configuration.
- Replace `174.138.76.103` in following command with the Public IP address of `Master Node`.
- Replace `174.138.76.107` in following command with the Public IP address of `Worker Node1`.
- Replace `174.138.76.108` in following command with the Public IP address of `Worker Node2`.

```
$ sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml


apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - name: kube-apiserver
    image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
    command:
    - /hyperkube
    - apiserver
    - --bind-address=0.0.0.0
    - --storage-backend=etcd2
    - --etcd-servers=http://174.138.76.103:2379,http://174.138.76.107:2379,http://174.138.76.108:2379
    - --allow-privileged=true
    - --service-cluster-ip-range=10.3.0.0/24
    - --secure-port=443
    - --advertise-address=174.138.76.103
    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
    - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
    - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
    - --client-ca-file=/etc/kubernetes/ssl/ca.pem
    - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
    - --runtime-config=extensions/v1beta1/networkpolicies=true
    - --anonymous-auth=false
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        port: 8080
        path: /healthz
      initialDelaySeconds: 15
      timeoutSeconds: 15
    ports:
    - containerPort: 443
      hostPort: 443
      name: https
    - containerPort: 8080
      hostPort: 8080
      name: local
    volumeMounts:
    - mountPath: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
      readOnly: true
    - mountPath: /etc/ssl/certs
      name: ssl-certs-host
      readOnly: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/ssl
    name: ssl-certs-kubernetes
  - hostPath:
      path: /usr/share/ca-certificates
    name: ssl-certs-host
```
### Set Up the kube-proxy Pod.
Kubernetes directs traffic from outside the cluster to the pods using proxies which are needed on every node. Create one on the master node using the yaml file below. 
```
$ sudo vi /etc/kubernetes/manifests/kube-proxy.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kube-proxy
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - name: kube-proxy
    image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
    command:
    - /hyperkube
    - proxy
    - --master=http://127.0.0.1:8080
    securityContext:
      privileged: true
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ssl-certs-host
      readOnly: true
  volumes:
  - hostPath:
      path: /usr/share/ca-certificates
    name: ssl-certs-host
```

### Set Up the kube-controller-manager Pod.
```
$ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml


apiVersion: v1
kind: Pod
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - name: kube-controller-manager
    image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
    command:
    - /hyperkube
    - controller-manager
    - --master=http://127.0.0.1:8080
    - --leader-elect=true
    - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
    - --root-ca-file=/etc/kubernetes/ssl/ca.pem
    resources:
      requests:
        cpu: 200m
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10252
      initialDelaySeconds: 15
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
      readOnly: true
    - mountPath: /etc/ssl/certs
      name: ssl-certs-host
      readOnly: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/ssl
    name: ssl-certs-kubernetes
  - hostPath:
      path: /usr/share/ca-certificates
    name: ssl-certs-host

```
### Set Up the kube-scheduler Pod.

```
$ sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - name: kube-scheduler
    image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
    command:
    - /hyperkube
    - scheduler
    - --master=http://127.0.0.1:8080
    - --leader-elect=true
    resources:
      requests:
        cpu: 100m
    livenessProbe:
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10251
      initialDelaySeconds: 15
      timeoutSeconds: 15
```
# Configure flannel Network IP Range

### Put the value of `SERVICE_IP_RANGE` Network configuration in ETCD store.
This CIDR network to use for service cluster IPs. Each service will be assigned a cluster IP out of this range. This must not overlap with any IP ranges assigned to the POD_NETWORK, or other existing network infrastructure. Routing to these IPs is handled by a proxy service local to each node, and are not required to be routable between nodes.

```
$ curl -X PUT -d "value={\"Network\":\"10.3.0.0/24\",\"Backend\":{\"Type\":\"vxlan\"}}" "http://174.138.76.108:2379/v2/keys/coreos.com/network/config"
```

### Put the value of `POD_IP_RANGE` Network configuration in ETCD store.
This CIDR network to use for pod IPs. Each pod launched in the cluster will be assigned an IP out of this range. Each node will be configured such that these IPs will be routable using the flannel overlay network.

```
$ etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16" }'
```

# Load Changed Units by reloading the Daemon.
```
$ sudo systemctl daemon-reload
```
# Start Flannel service.
Start the Flannel service. Check the status of Flannel service and ake sure that Flannel service will start after boot so enable the Flannel service.
```
$ sudo systemctl start flanneld
$ sudo systemctl status flanneld
$ sudo systemctl enable flanneld
```
# Start kubelet service.
With all of the configuration and manifest files in place, start the kubelet service and Check the staus of kubelet service. Make sure that kubelet service will start after boot so make enable the kubelet service.
```
$ sudo systemctl start kubelet
$ sudo systemctl status kubelet
$ sudo systemctl enable kubelet
```

# Basic Health-Checks.
First, we need to make sure that the Kubernetes API is available (wait for few minutes after starting the kubelet service).

```
$ curl http://127.0.0.1:8080/version

{
  "major": "1",
  "minor": "6",
  "gitVersion": "v1.6.1+coreos.0",
  "gitCommit": "9212f77ed8c169a0afa02e58dce87913c6387b3e",
  "gitTreeState": "clean",
  "buildDate": "2017-04-04T00:32:53Z",
  "goVersion": "go1.7.5",
  "compiler": "gc",
  "platform": "linux/amd64"
}
```
 Once the kubelet service is up and running, you can check its pods via the metadata api.
```
$ curl -s localhost:10255/pods | jq -r '.items[].metadata.name'
 
kube-controller-manager-174.138.76.103
kube-proxy-174.138.76.103
kube-scheduler-174.138.76.103
kube-apiserver-174.138.76.103
```
This Shows our Master Node of kubernetes is properly configured.
